# -*- coding: utf-8 -*-
"""Untitled.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hBrFGNToiCrIKNqTzShE3yX1p4xWKSeP
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install --upgrade geopandas
!pip install --upgrade pyshp
!pip install --upgrade shapely
!pip install --upgrade descartes
!pip install --upgrade geoplot
!apt-get install libgeos-3.5.0
!apt-get install libgeos-dev
!pip install https://github.com/matplotlib/basemap/archive/master.zip
!pip install pyproj==1.9.6
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
# %matplotlib inline
# %matplotlib inline
import pandas as pd

!pip install -q pydot

!pip install --upgrade geopandas

import geopandas as gpd
import matplotlib.pyplot as plt
import requests
import zipfile
from shapely.geometry import Point

from geopandas import GeoDataFrame

import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import ADASYN 
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn import datasets
from sklearn.preprocessing import LabelEncoder  # For categorical data
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# Import tools needed for visualization
from sklearn.tree import export_graphviz
from sklearn.decomposition import PCA
import pydot
import seaborn as sns
import itertools

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
from collections import OrderedDict
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_excel('/content/drive/My Drive/NASA Challenge/FINAL NASA ALL/Global_Landslide_Catalog-Export.xlsx')

df.head(5)

df.info()

missing_df = df.isnull().sum(axis=0).reset_index()
missing_df.columns = ['variable', 'missing values']
missing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100
missing_df.sort_values('filling factor (%)').reset_index(drop = True)

df.shape # See how many data objects & attributes we have

df.landslide_trigger.unique() # To get the distinct value for landslide_trigger column

dataframe=df

del df['event_time'] # Deleting event_time column has there was no any data in that column.

############################### DATA PREPROCESSING ###############################
# In this section I performed data preprocessing by removing noise, replacing NaN
# values, replacing NaN/empty values with the median of that attribute and dropping
# columns that wouldn't help our model train
##################################################################################
# Remove unwanted columns
df = df.drop(columns=['event_title',
                      'event_id','event_description',
                      'gazeteer_closest_point', 'country_code','gazeteer_distance','last_edited_date'])

df = df.dropna(subset=['landslide_trigger'])
df=df.dropna(subset=["country_name"]) # remove null vrows of landslide_trigger

missing_df = df.isnull().sum(axis=0).reset_index()
missing_df.columns = ['variable', 'missing values']
missing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100
missing_df.sort_values('filling factor (%)').reset_index(drop = True)

# Remove unwanted columns
df = df.drop(columns=['notes',
                      'storm_name','photo_link'])

# Replace NaN values in fatality & injury count with the medians
df['fatality_count'].fillna((df['fatality_count'].median()), inplace=True)
df['injury_count'].fillna((df['injury_count'].median()), inplace=True)

df.isnull().sum() # check for missing values in dataset

df.landslide_size.unique()

#Delete attribute values that we don't want to classify:
df = df[df.landslide_size != "catastrophic"]
df = df[df.landslide_category != "unknown"]
df = df[df.landslide_category != "other"]
df = df[df.country_name != "NaN"]

missing_df = df.isnull().sum(axis=0).reset_index()
missing_df.columns = ['variable', 'missing values']
missing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100
missing_df.sort_values('filling factor (%)').reset_index(drop = True)

df.landslide_size.unique()

df["landslide_size"].fillna("small", inplace = True)

df = df.dropna(subset=['location_description'])

del df['landslide_setting']
del df['submitted_date']
del df['created_date']
# deleting redundant column as eventdate covers

missing_df = df.isnull().sum(axis=0).reset_index()
missing_df.columns = ['variable', 'missing values']
missing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100
missing_df.sort_values('filling factor (%)').reset_index(drop = True)

df["admin_division_name"].fillna("NoDIVISION", inplace = True)
df['admin_division_population'].fillna((df['admin_division_population'].median()), inplace=True)
df["event_import_source"].fillna("ABCSource", inplace = True)
df['event_import_id'].fillna((df['event_import_id'].median()), inplace=True)
df["source_link"].fillna("ABCLink", inplace = True)
df["location_accuracy"].fillna("1km", inplace = True)

# Print graph showing imbalanced data:
# Changing into approriate integer
df["landslide_trigger"].replace({"unknown": "downpour"}, inplace=True)
pd.value_counts(df['landslide_trigger']).plot.bar()
plt.title('Landslide Trigger Distribution')
plt.xlabel('Class')
plt.ylabel('Data Objects')
df['landslide_trigger'].value_counts()

geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]
gdf = GeoDataFrame(df, geometry=geometry)   

#this is a simple map that goes with geopandas
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
gdf.plot(ax=world.plot(figsize=(20, 10)), marker='o', color='red', markersize=15);

# Make sure all categorical data is string value only

df['landslide_category'] = df['landslide_category'].astype(str) 
df['country_name'] = df['country_name'].astype(str) #make sure all country name values are strings

# Label Encode our categorical data with dummy variables
#df = pd.get_dummies(df, prefix=['country_name','landslide_category','landslide_size'], 
 #                   columns=['country_name','landslide_category','landslide_size'])

df['country_name'].nunique()

df['country_name'].value_counts()

count_countrywise = df['country_name'].value_counts()

country_df = pd.DataFrame(count_countrywise)

country_df

df['Country_Name']=df['country_name']
df['Country_Name']

country_df.rename(columns = {'country_name': 'Landslide_Count'}, inplace = True)

top10_country= country_df.sort_values('Landslide_Count', ascending=False).head(10)

df.head()

df.info()

plt.figure(figsize=(12,6))
plt.ylabel('Country')
#plt.xlabel('Number of Landslide Cases')
plt.title('Top 10 countries on the basis of Landslide Occurences')
sns.barplot(top10_country['Landslide_Count'], top10_country.index);

"""# ** Above Graph Shows the Top 10 Countries on the basis of Landslide Occurence**

### Landslide Count of TOP 10 Countries
"""

count_countrywise.head(10)

cols = ['landslide_size','landslide_category','country_name','landslide_trigger','fatality_count','injury_count','event_date']
df1 =pd.DataFrame(df, columns = cols)

df1.sample(5)

df.head()



plt.figure(figsize=(30,15))
plt.title('Landslide Size Vs Countries')
sns.swarmplot(df['landslide_size'], df['Country_Name']);

plt.figure(figsize=(12,6))
plt.title('Landslide Size Vs Countries')
sns.swarmplot(df1['landslide_size'], df1['country_name']);

missing_df = df.isnull().sum(axis=0).reset_index()
missing_df.columns = ['variable', 'missing values']
missing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100
missing_df.sort_values('filling factor (%)').reset_index(drop = True)

df['landslide_trigger']

# Label Encode our trigger values into numbers
df["landslide_trigger"] = df["landslide_trigger"].astype('category')
df["landslide_trigger"] = df["landslide_trigger"].cat.codes
df.head(5)

"""Here we changed landslide trigger value that was string into int : Keep Note 

---

**rain= 11 , downpour= 3 , monsoon = 8 , tropical_cyclone = 143,....**
"""

df.landslide_trigger.nunique()

df['landslide_trigger'].unique()

df['location_accuracy'].unique()

# Changing into approriate integer
df["location_accuracy"].replace({"unknown": "0", "5km": "5","10km":"10","25km":"25","exact":"0","1km":"1","50km":"50","250km":"250","100km":"100"}, inplace=True)

df["location_accuracy"]

# Label Encode our divisioname values into numbers
df["admin_division_name"] = df["admin_division_name"].astype('category')
df["admin_division_name"] = df["admin_division_name"].cat.codes


# Label Encode our event_import_sourcer values into numbers
df["event_import_source"] = df["event_import_source"].astype('category')
df["event_import_source"] = df["event_import_source"].cat.codes


# Label Encode our source_link values into numbers
df["source_link"] = df["source_link"].astype('category')
df["source_link"] = df["source_link"].cat.codes


# Label Encode our Location Description values into numbers
df["location_description"] = df["location_description"].astype('category')
df["location_description"] = df["location_description"].cat.codes
df.head(5)

# Label Encode our SourceName values into numbers
df["source_name"] = df["source_name"].astype('category')
df["source_name"] = df["source_name"].cat.codes

# Label Encode our Locationaccuracy values into numbers
df["location_accuracy"] = df["location_accuracy"].astype('category')
df["location_accuracy"] = df["location_accuracy"].cat.codes
df.head(5)

df['location_description'].value_counts()



df.info()

df['event_year'] = df['event_date'].dt.year
df['event_month'] = df['event_date'].dt.month
df['event_week'] = df['event_date'].dt.week
df['event_day'] = df['event_date'].dt.day
#df['event_date_hour'] = df['event_date'].dt.hour
#df['event_date_minute'] = df['event_date'].dt.minute
#df['event_Date'] = df['event_date'].dt.date'''

import datetime as dt

df['event_year']

df_by_year = df.sort_values('event_year');

df_by_year = df_by_year.set_index('event_year')

plt.figure(figsize=(12,8))
plt.title('Fatalities,Injuries vs Year')
plt.xlabel('Year')
plt.ylabel('Fatality/Injury Count')
plt.plot( df_by_year.index, df_by_year['fatality_count'], 's-b')
plt.plot( df_by_year.index, df_by_year['injury_count'], 'o-r');

df.sample(2)

#country vs fatality count
col1 = ['country_name','landslide_category', 'landslide_trigger','landslide_size','fatality_count', 'injury_count','admin_division_population','event_year' ]
df2 = pd.DataFrame(df, columns = col1)

#country vs fatality count
CbyF = df2.groupby('country_name')[['fatality_count']].sum()
print(CbyF)

Countrydeath_count = CbyF.sort_values('fatality_count', ascending = False).head(20)

"""### **The below list shows the 10 countries with maximum deaths**"""

#Which countries have the maximum reported deaths?
Countrydeath_count.head(10)
#the below list shows the 10 countries with maximum deaths

plt.figure(figsize=(12,8))
plt.title('Top 20 countries with maximum death toll')
sns.heatmap(Countrydeath_count, cmap = 'OrRd_r');

#country vs injury_count 
CbyI = df2.groupby('country_name')[['injury_count']].sum()
print(CbyI)

Countryinjury_count = CbyI.sort_values('injury_count', ascending = False).head(20)

"""#**MODEL** **BUILDING**

#**The list below shows the top 10 countries that had maximum injuries **
"""

Countryinjury_count.head(10)

plt.figure(figsize=(12,8))
plt.title('Top 20 countries with maximum injuries')
sns.heatmap(Countryinjury_count, cmap = 'BuPu_r');

dataframe.head()



df.head()

# Split our labels into their own array
#Here Our Target class :  landslide trigger 
Y = np.array(df['landslide_trigger'])  # values we want to predict

Y

"""**# Target Class= landslide_trigger**"""

# Remove the labels from the features, axis 1 refers to the col
X = df.drop('landslide_trigger',axis=1)

df.columns

del df['country_name']

df['event_date_hour'] = df['event_date'].dt.hour
df['event_date_minute'] = df['event_date'].dt.minute

feature_list = list(df.columns)
feature_list

feature_list = list(df.columns)
feature_list

X=df

df.dtypes

#Converting float type to int 
df['injury_count'] = df['injury_count'].astype(int) 
df['latitude'] = df['latitude'].astype(int) 
df['longitude'] = df['longitude'].astype(int) 
df['fatality_count'] = df['fatality_count'].astype(int) 
df['event_import_id'] = df['event_import_id'].astype(int) 
df['admin_division_population'] = df['admin_division_population'].astype(int) 
df.dtypes

del df['geometry']

#Removing Redundant Column
del df['event_date']

feature_list = list(df.columns)
feature_list

df.head()

#For any graphs to be added creating duplicate dataframe
df2=df

df.dtypes

# Label Encode our landslide_category values into numbers
df["landslide_category"] = df["landslide_category"].astype('category')
df["landslide_category"] = df["landslide_category"].cat.codes





# Label Encode our Country_Name  values into numbers
df["Country_Name"] = df["Country_Name"].astype('category')
df["Country_Name"] = df["Country_Name"].cat.codes

# Label Encode our landslide_size values into numbers
df["landslide_size"] = df["landslide_size"].astype('category')
df["landslide_size"] = df["landslide_size"].cat.codes

df.head()

feature_list = list(df.columns)
feature_list

df.dtypes

features=df[['source_link','location_description','location_accuracy','landslide_category','landslide_size','fatality_count','injury_count','event_import_source','event_import_id','admin_division_name','admin_division_population','longitude','latitude','Country_Name','event_year','event_month','event_week','event_day','event_date_hour','event_date_minute','landslide_size']]

duplicate=df

writer = pd.ExcelWriter('/content/drive/My Drive/NASA Challenge/FINAL NASA ALL/AfterProcessing.xlsx')
duplicate.to_excel(writer)
writer.save()

Y

print(type (Y))

print(type (feature_list))

X_train, X_test, y_train, y_test = train_test_split(features,Y, test_size=0.3,random_state=20000)

X.shape

features.shape



"""# Decision Tree #"""

#fit a classification tree with max_depth=30 on all data
from sklearn.tree import DecisionTreeClassifier
treeclf=DecisionTreeClassifier(max_depth=30,random_state=1)
treeclf.fit(X_train,y_train)
Y_pred1=treeclf.predict(X_test)

from sklearn import metrics
print("Accuracy: ", metrics.accuracy_score(y_test,Y_pred1))

df.to_csv('preprocessed.csv')
!cp preprocessed.csv "/content/drive/My Drive/NASA Challenge/FINAL NASA ALL/"

features.to_csv('training.csv')
!cp training.csv "/content/drive/My Drive/NASA Challenge/FINAL NASA ALL/"

"""Predicting the landslide trigger of Nepal with death=6, injury=2 for date 2020/10/**4**"""

prediction= treeclf.predict([[600,3743,1,6,1,5,3,2,672,600,5,75,9,86,2020,10,2,4,10,20,1]])

print(prediction)

"""THIS 11 Denotes that the cause for landslide can be due to landslide_trigger(11) i.e rain"""

prediction1=treeclf.predict([[4830,3743,6,6,2,0,0,1,936,552,36619,-122,45,134,2020,10,1,30,2,0,2]])

print(prediction1)

"""Similary, the model is predicting the cause for country 134 landslide cause is 12 i.e

# Decision Tree Accuracy : 67%
"""

X=features

"""# Random Forest"""

train_features, test_features, train_labels, test_labels = train_test_split(X, Y, test_size =.30)

#fit a random forest classifier
from sklearn.ensemble import RandomForestClassifier
rfc1=RandomForestClassifier(max_depth=30,min_samples_split=2,max_features=0.90)
rfc1.fit(train_features, train_labels)

#from sklearn.ensemble import RandomForestClassifier
#rf=RandomForestClassifier(max_depth = 5, n_estimators=100, random_state = 42)
#rf.fit(train_features, train_labels)

y_train_preds = rfc1.predict_proba(train_features)[:,1]
y_valid_preds = rfc1.predict_proba(test_features)[:,1]

Y_pred = rfc1.predict(test_features)

from sklearn import metrics
metrics.accuracy_score(test_labels,Y_pred)

"""# Random Forest Tree Accuracy : 77.46 %"""

# Now Predicting from random Forest
predictionrandom= rfc1.predict([[600,3743,1,6,1,5,3,2,672,600,5,75,9,86,2020,10,2,4,10,20,1]])

print(predictionrandom)

"""# Its Predicting Landslide for above data is due to Landslide_trigger cause i.e 3 which means **downpour**"""

from sklearn.tree import DecisionTreeClassifier

from sklearn import tree

clf = tree.DecisionTreeClassifier()
fig, ax = plt.subplots(figsize=(12, 12))
tree.plot_tree(clf.fit(X, Y), max_depth=4, fontsize=10)
plt.show()

data_feature_names=['source_link','location_description','location_accuracy','landslide_category','landslide_size','fatality_count','injury_count','event_import_source','event_import_id','admin_division_name','admin_division_population','longitude','latitude','Country_Name','event_year','event_month','event_week','event_day','event_date_hour','event_date_minute','landslide_size']

import pydotplus
import collections

# Visualize data
dot_data = tree.export_graphviz(clf,
                                feature_names=data_feature_names,
                                out_file=None,
                                filled=True,
                                rounded=True)
graph = pydotplus.graph_from_dot_data(dot_data)

colors = ('turquoise', 'orange')
edges = collections.defaultdict(list)

for edge in graph.get_edge_list():
    edges[edge.get_source()].append(int(edge.get_destination()))

for edge in edges:
    edges[edge].sort()    
    for i in range(2):
        dest = graph.get_node(str(edges[edge][i]))[0]
        dest.set_fillcolor(colors[i])

graph.write_png('/content/drive/My Drive/NASA Challenge/FINAL NASA ALL/tree.pdf')















