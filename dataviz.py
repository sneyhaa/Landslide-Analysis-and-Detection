# -*- coding: utf-8 -*-
"""DataViz.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fObIHV-y1-g-sH09UIedWE4a-YRz45Md
"""





# Commented out IPython magic to ensure Python compatibility.
!pip install --upgrade geopandas
!pip install --upgrade pyshp
!pip install --upgrade shapely
!pip install --upgrade descartes
!pip install --upgrade geoplot
!apt-get install libgeos-3.5.0
!apt-get install libgeos-dev
!pip install https://github.com/matplotlib/basemap/archive/master.zip
!pip install pyproj==1.9.6
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
# %matplotlib inline
# %matplotlib inline
import pandas as pd

!pip install --upgrade geopandas

import geopandas as gpd
import matplotlib.pyplot as plt
import requests
import zipfile
from shapely.geometry import Point

from geopandas import GeoDataFrame

import numpy as np
import pandas as pd
from imblearn.over_sampling import SMOTE
from imblearn.over_sampling import ADASYN 
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn import datasets
from sklearn.preprocessing import LabelEncoder  # For categorical data
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

# Import tools needed for visualization
from sklearn.tree import export_graphviz
from sklearn.decomposition import PCA
import pydot
import seaborn as sns
import itertools

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt
from collections import OrderedDict
import warnings
warnings.filterwarnings('ignore')

import pandas as pd

from google.colab import drive
drive.mount("/content/gdrive")

#df = pd.read_excel('https://drive.google.com/drive/my-drive/Global_Landslide_Catalog-Export.xlsx')

import pandas as pd
df = pd.read_excel('/content/gdrive/My Drive/Landslide data/Global_Landslide_Catalog-Export.xlsx')



df.head(5)

df.columns

df.info()

missing_df = df.isnull().sum(axis=0).reset_index()
missing_df.columns = ['variable', 'missing values']
missing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100
missing_df.sort_values('filling factor (%)').reset_index(drop = True)

df.shape # See how many data objects & attributes we have

df.landslide_trigger.unique() # To get the distinct value for landslide_trigger column

#del df['event_time'] # Deleting event_time column has there was no any data in that column.

############################### DATA PREPROCESSING ###############################
# In this section I performed data preprocessing by removing noise, replacing NaN
# values, replacing NaN/empty values with the median of that attribute and dropping
# columns that wouldn't help our model train
##################################################################################
# Remove unwanted columns
df = df.drop(columns=['event_title',
                      'event_id','event_description',
                      'gazeteer_closest_point', 'country_code','gazeteer_distance','last_edited_date'])



df = df.dropna(subset=['landslide_trigger']) # remove null vrows of landslide_trigger

missing_df = df.isnull().sum(axis=0).reset_index()
missing_df.columns = ['variable', 'missing values']
missing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100
missing_df.sort_values('filling factor (%)').reset_index(drop = True)

# Remove unwanted columns
df = df.drop(columns=['notes',
                      'storm_name','photo_link'])

# Replace NaN values in fatality & injury count with the medians
df['fatality_count'].fillna((df['fatality_count'].median()), inplace=True)
df['injury_count'].fillna((df['injury_count'].median()), inplace=True)
df.isnull().sum() # check for missing values in dataset

df.landslide_size.unique()

#Delete attribute values that we don't want to classify:
df = df[df.landslide_size != "catastrophic"]
df = df[df.landslide_category != "unknown"]
df = df[df.landslide_category != "other"]
df = df[df.country_name != "NaN"]

missing_df = df.isnull().sum(axis=0).reset_index()
missing_df.columns = ['variable', 'missing values']
missing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100
missing_df.sort_values('filling factor (%)').reset_index(drop = True)



df.landslide_size.unique()

# replacing na values in college with No college 
df["landslide_size"].fillna("small", inplace = True)

df = df.dropna(subset=['location_description'])

del df['landslide_setting']
del df['submitted_date']
del df['created_date']
# deleting redundant column as eventdate covers

missing_df = df.isnull().sum(axis=0).reset_index()
missing_df.columns = ['variable', 'missing values']
missing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100
missing_df.sort_values('filling factor (%)').reset_index(drop = True)

df["admin_division_name"].fillna("NoDIVISION", inplace = True)
df['admin_division_population'].fillna((df['admin_division_population'].median()), inplace=True)
df["event_import_source"].fillna("ABCSource", inplace = True)
df['event_import_id'].fillna((df['event_import_id'].median()), inplace=True)
df["source_link"].fillna("ABCLink", inplace = True)
df["location_accuracy"].fillna("1km", inplace = True)

# Print graph showing imbalanced data:
# Changing into approriate integer
df["landslide_trigger"].replace({"unknown": "downpour"}, inplace=True)
pd.value_counts(df['landslide_trigger']).plot.bar()
plt.title('Landslide Trigger Distribution')
plt.xlabel('Class')
plt.ylabel('Data Objects')
df['landslide_trigger'].value_counts()

geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]
gdf = GeoDataFrame(df, geometry=geometry)   

#this is a simple map that goes with geopandas
world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))
gdf.plot(ax=world.plot(figsize=(20, 10)), marker='o', color='red', markersize=15);

# Make sure all categorical data is string value only

df['landslide_category'] = df['landslide_category'].astype(str) 
df['country_name'] = df['country_name'].astype(str) #make sure all country name values are strings

# Label Encode our categorical data with dummy variables
#df = pd.get_dummies(df, prefix=['country_name','landslide_category','landslide_size'], 
                    #columns=['country_name','landslide_category','landslide_size'])

df['country_name'].nunique()
#there are 142 countries

df['country_name'].value_counts()

df['country_name'].replace({'nan':'Country Unkown'}, inplace = True)

count_countrywise = df['country_name'].value_counts()

count_countrywise = count_countrywise.drop('Country Unkown', axis = 0)

country_df = pd.DataFrame(count_countrywise)

country_df = country_df.drop('Country Unkown', axis=0)

country_df.rename(columns = {'country_name': 'Landslide_Count'}, inplace = True)



top10_country= country_df.sort_values('Landslide_Count', ascending=False).head(10)

plt.figure(figsize=(12,6))
plt.ylabel('Country')
#plt.xlabel('Number of Landslide Cases')
plt.title('Top 10 countries on the basis of Landslide Occurences')
sns.barplot(top10_country['Landslide_Count'], top10_country.index);

#What are the top 10 countries with maximum landslide occurences?

count_countrywise.head(10)

cols = ['landslide_size','landslide_category','country_name','landslide_trigger','fatality_count','injury_count','event_date']
df1 =pd.DataFrame(df, columns = cols)

df1["country_name"] = df1["country_name"].astype('category')
df1["country_name"] = df1["country_name"].cat.codes

df1.sample(5)

plt.figure(figsize=(12,6))
plt.title('Landslide Size Vs Countries')
sns.swarmplot(df1['landslide_size'], df1['country_name']);

df1.info()

#sns.barplot(df1['landslide_category'], df1['landslide_trigger']);

df['landslide_trigger']

# Label Encode our trigger values into numbers
df["landslide_trigger"] = df["landslide_trigger"].astype('category')
df["landslide_trigger"] = df["landslide_trigger"].cat.codes
df.head(5)

"""Here we changed landslide trigger value that was string into int : Keep Note **rain= 12 , downpour= 3 , monsoon = 9 , tropical_cyclone = 14 ,....**"""

df['location_accuracy'].unique()

# Changing into approriate integer
df["location_accuracy"].replace({"unknown": "0", "5km": "5","10km":"10","25km":"25","exact":"0","1km":"1","50km":"50","250km":"250","100km":"100"}, inplace=True)

df["location_accuracy"]

# Label Encode our divisioname values into numbers
df["admin_division_name"] = df["admin_division_name"].astype('category')
df["admin_division_name"] = df["admin_division_name"].cat.codes


# Label Encode our event_import_sourcer values into numbers
df["event_import_source"] = df["event_import_source"].astype('category')
df["event_import_source"] = df["event_import_source"].cat.codes


# Label Encode our source_link values into numbers
df["source_link"] = df["source_link"].astype('category')
df["source_link"] = df["source_link"].cat.codes


# Label Encode our Location Description values into numbers
df["location_description"] = df["location_description"].astype('category')
df["location_description"] = df["location_description"].cat.codes
df.head(5)

# Label Encode our SourceName values into numbers
df["source_name"] = df["source_name"].astype('category')
df["source_name"] = df["source_name"].cat.codes

# Label Encode our Locationaccuracy values into numbers
df["location_accuracy"] = df["location_accuracy"].astype('category')
df["location_accuracy"] = df["location_accuracy"].cat.codes
df.head(5)

df['location_description'].value_counts()

df.info()

df['landslide_trigger'].dtype

df['source_name'].dtypes

df.dtypes

##df['geometry'] #drop this #graphs plot garna milcha
#event date

#Geometry info
#sns.scatterplot(x = 'geometry', y = lan

df.sample(3)

df.columns

df.info()

df.head(2)

df['event_year'] = df['event_date'].dt.year
df['event_month'] = df['event_date'].dt.month
df['event_week'] = df['event_date'].dt.week
df['event_day'] = df['event_date'].dt.day
#df['event_date_hour'] = df['event_date'].dt.hour
#df['event_date_minute'] = df['event_date'].dt.minute
#df['event_Date'] = df['event_date'].dt.date'''

import datetime as dt

df['event_year']

df_by_year = df.sort_values('event_year');

df_by_year = df_by_year.set_index('event_year')

plt.figure(figsize=(12,8))
plt.title('Fatalities,Injuries vs Year')
plt.xlabel('Year')
plt.ylabel('Fatality/Injury Count')
plt.plot( df_by_year.index, df_by_year['fatality_count'], 's-b')
plt.plot( df_by_year.index, df_by_year['injury_count'], 'o-r');

df.sample(2)



df['landslide_category'].unique()

col22 = ['admin_division_population', 'country_name','fatality_count']
df33 = pd.DataFrame(df, columns = col22)

df33.head(3)

dfpop = df33.groupby('country_name')[['admin_division_population','fatality_count']].sum()

dfpop.sample(5)

sns.lmplot(y ='admin_division_population', x = 'fatality_count', data = dfpop);



#country vs fatality count
col1 = ['country_name','landslide_category', 'landslide_trigger','landslide_size','fatality_count', 'injury_count','admin_division_population','event_year' ]
df2 = pd.DataFrame(df, columns = col1)

#df2 = df2.drop('Country Unkown', axis = 0);

#country vs fatality count
CbyF = df2.groupby('country_name')[['fatality_count']].sum()

Countrydeath_count = CbyF.sort_values('fatality_count', ascending = False).head(21)

Countrydeath_count = Countrydeath_count.drop('Country Unkown', axis = 0)

#Which countries have the maximum reported deaths?
Countrydeath_count.head(10)
#the below list shows the 10 countries with maximum deaths

plt.figure(figsize=(12,8))
plt.title('Top 20 countries with maximum death toll')
sns.heatmap(Countrydeath_count, cmap = 'OrRd_r');

#country vs injury
CbyI = df2.groupby('country_name')[['injury_count']].sum()

Countryinjury_count = CbyI.sort_values('injury_count', ascending = False).head(20)

Countryinjury_count = Countryinjury_count.drop('Country Unkown', axis = 0)

Countryinjury_count.head(10)
#the list below shows the top 10 countries that had maximum injuries

plt.figure(figsize=(12,8))
plt.title('Top 20 countries with maximum injuries')
sns.heatmap(Countryinjury_count, cmap = 'BuPu_r');

'''plt.figure(figsize=(12,6))
plt.title('Fatalities over the years')
sns.barplot( df_by_year.index, df_by_year['fatality_count']);'''

#df['event_date_year']

# Split our labels into their own array
#Here Our Target class :  landslide trigger 
#Y = np.array(df['landslide_trigger'])  # values we want to predict

# Remove the labels from the features, axis 1 refers to the col
#df = df.drop('landslide_trigger',axis=1)

df1.info()

df1.landslide_size.unique()

df.location_description.dtype

df2.head(2)

df2.admin_division_population.dtype

#plt.plot(df2.admin_division_population, df2.injury_count);
#g = sns.catplot("alive", col="deck", col_wrap=4,
                #data=titanic[titanic.deck.notnull()],
                #kind="count", height=3.5, aspect=.8, 
                #palette='tab20')

#fig.suptitle('sf')
df.plot(kind='pie', subplots=True, figsize=(8, 8), dpi= 80)

df2.head(2)

plt.figure(figsize=(12,6))
sns.scatterplot(df2['admin_division_population'], df2['fatality_count'], hue = df2['landslide_size'], s = 100)
plt.title('Fatality vs Population (by landslide size)')
plt.xlabel('Fatality')
plt.ylabel('Population');

dfyearr = pd.DataFrame(df, columns = ['event_year','fatality_count'])

dfdeath_year = dfyearr.sort_values('event_year')

dfdeath_year.head(3)

deathyear_df = dfyearr.groupby('event_year').sum()

deathyear_df.head(20)

plt.figure(figsize=(12,6))
sns.lineplot(deathyear_df.index, deathyear_df['fatality_count'])
plt.title('Fatality vs Year');

dfdeath_year.set_index('event_year')



#sns.barplot(dfdeath_year.index, dfdeath_year.fatality_count);

df3 = df2.groupby('landslide_size').size()

df3.plot(kind = 'pie', subplots = True, figsize =(8,8), autopct='%1.1f%%', startangle=15, shadow = True); #colors = my_colors);
plt.title("Pie Chart of Landslide Sizes")
plt.xlabel(" ")
plt.ylabel('');

df_by_year.head(3)

from statsmodels.tsa.seasonal import seasonal_decompose

from dateutil.parser import parse

df_by_year['event_date']

df_by_year.set_index('event_date', inplace=True)

pip install calmap

import calmap

df_by_year.head(2)



df['event_year'] = pd.DatetimeIndex(df.event_date).year

df_by_year['eventyear']= df['event_year']

colss = ['event_date','event_day','event_week','event_month','event_year','fatality_count','injury_count']
df_time = pd.DataFrame(df_by_year, columns = colss)



df_time.head(3)

#plt.figure(figsize=(16,10), dpi= 80)
#calmap.calendarplot(df_time['2014']['VIX.Close'], fig_kws={'figsize': (16,10)}, yearlabel_kws={'color':'black', 'fontsize':14}, subplot_kws={'title':'Yahoo Stock Prices'})
#plt.show()

df_time

df_month = df_time.groupby('event_month')[['fatality_count','injury_count']].count()





plt.figure(figsize=(12,6))
plt.plot(df_month.index, df_month['fatality_count'], 'o-b')
plt.title('Fatalities vs Months');
plt.xlabel('Month')
#plt.grid(False)
plt.ylabel('Fatality count');



plt.figure(figsize=(12,6))
plt.plot(df_month.index, df_month['injury_count'], 's-r')
plt.title('Injuries vs Months');
plt.xlabel('Month')
#plt.grid(False)
plt.ylabel('Injury count');









